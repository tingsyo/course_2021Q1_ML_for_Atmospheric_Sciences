{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997b6652",
   "metadata": {},
   "source": [
    "# Using Generalized Linear Model with Feature Vectors for Event Detections\n",
    "\n",
    "Finally, we are now using the **feature vectors** described in the [previous notebook](https://github.com/tingsyo/dlgridsat/blob/main/notebook/08_DataFlow_for_Experiments.ipynb) along with the [**Generalized Linear Model**](https://en.wikipedia.org/wiki/Generalized_linear_model) to detect and to predict the events described in the [other notebook](https://github.com/tingsyo/dlgridsat/blob/main/notebook/00_weather_events.ipynb).\n",
    "\n",
    "\n",
    "## Define the terms\n",
    "\n",
    "### Weather Events\n",
    "\n",
    "- **HRD**: Precip >= 40mm/day\n",
    "- **HRH**: Precip >= 10mm/hr\n",
    "- **CS**: 寒潮，台北測站24小時內有任一小時10度C以下\n",
    "- **TYW**: 中央氣象局發布颱風警報\n",
    "- **NWPTC**: 西北太平洋地區有熱帶氣旋\n",
    "- **FT**: 中央氣象局地面天氣圖，2000年以後以00Z代表\n",
    "- **NE**: 彭佳嶼測站日平均風向為東北風(15-75度)及風速達4m/s\n",
    "- **SWF**: CFSR 850hPa 紅色區域內 u平均>0並且v平均>0並且平均風達3m/s 或者 >6m/s的風速範圍站紅色區域30%\n",
    "\n",
    "\n",
    "### Feature Vectors\n",
    "\n",
    "- **PCA** : Principle Component Analysis with 2,048 components.\n",
    "- **CAE** : Convolutional Auto-Encoder encoded vectors, with a length of 2,048.\n",
    "- **CVAE**: Variational Auto-Encoder encoded vectors, with a length of 2,048.\n",
    "- **PTBE**: [ResNet50 pre-trained with Big-Earth dataset](https://tfhub.dev/google/remote_sensing/bigearthnet-resnet50/1), with a length of 2,048.\n",
    "- **PTIN**: [ResNet50 pre-trained with ImageNet dataset](https://tfhub.dev/tensorflow/resnet_50/feature_vector/1), with a length of 2,048.\n",
    "\n",
    "\n",
    "## Data Partition\n",
    "\n",
    "We keep the year 2016 for validation, and 2013~2015 for training and development. For Training and development, we use cross-validation for model tuning.\n",
    "\n",
    "### Event Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eace233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CS  TYW  NWPTY  FT  NE  SWF  HRD  HRH\n",
      "20130101   0  0.0      1   0   0    0    1    0\n",
      "20130102   0  0.0      1   0   1    0    0    0\n",
      "20130103   0  0.0      1   0   1    0    1    1\n",
      "20130104   0  0.0      1   0   1    0    1    0\n",
      "20130105   0  0.0      1   0   1    0    1    1\n",
      "(1461, 8)\n",
      "CS\t counts: 12\t prob:0.008213552361396304\n",
      "TYW\t counts: 65.0\t prob:0.044490075290896644\n",
      "NWPTY\t counts: 702\t prob:0.4804928131416838\n",
      "FT\t counts: 244\t prob:0.16700889801505817\n",
      "NE\t counts: 471\t prob:0.32238193018480493\n",
      "SWF\t counts: 406\t prob:0.2778918548939083\n",
      "HRD\t counts: 420\t prob:0.2874743326488706\n",
      "HRH\t counts: 520\t prob:0.35592060232717315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "\n",
    "# Read all events\n",
    "events = pd.read_csv('../data/tad_filtered.csv', index_col=0)\n",
    "\n",
    "print(events.head())\n",
    "print(events.shape)\n",
    "for c in events.columns:\n",
    "    print(c + '\\t counts: ' + str(events[c].sum()) + '\\t prob:' + str(events[c].sum()/events.shape[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa923b",
   "metadata": {},
   "source": [
    "## Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1ed9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0          1         2         3         4         5  \\\n",
      "20130101  10.258309  -4.656837 -3.172782  0.632345  2.013453  3.179133   \n",
      "20130102   8.503345  -6.036067 -2.736589 -3.880598  2.831698  2.121468   \n",
      "20130103   7.555368 -10.951109 -2.391194 -0.284879  2.710904 -1.378869   \n",
      "20130104   7.380538 -10.050537 -3.675916  1.638377  2.332644  0.610044   \n",
      "20130105   7.389496  -7.858804 -3.157701 -1.307389  2.557700  0.431949   \n",
      "\n",
      "                 6         7         8         9  ...      2038      2039  \\\n",
      "20130101 -1.438060  0.716551  3.222975 -1.491041  ... -0.066616  0.034927   \n",
      "20130102  1.351488  1.792430 -0.812001 -1.764388  ... -0.019680 -0.009342   \n",
      "20130103  0.165945  2.193658 -2.956463  0.340051  ... -0.026055  0.284996   \n",
      "20130104  0.575185  2.279190 -0.802140 -1.236430  ... -0.106318  0.014793   \n",
      "20130105 -0.242018  2.094918  0.823646 -2.035190  ... -0.031707  0.090493   \n",
      "\n",
      "              2040      2041      2042      2043      2044      2045  \\\n",
      "20130101  0.031727 -0.031884 -0.113242  0.003574  0.017633 -0.026924   \n",
      "20130102 -0.068745 -0.176662 -0.094584  0.095433  0.125961 -0.058037   \n",
      "20130103  0.240756 -0.122560  0.034209  0.065936  0.090867  0.119136   \n",
      "20130104  0.064468  0.017858 -0.133863  0.055337 -0.077100 -0.047909   \n",
      "20130105  0.121777  0.096001 -0.173047 -0.111648  0.134817 -0.078974   \n",
      "\n",
      "              2046      2047  \n",
      "20130101  0.068095  0.065840  \n",
      "20130102 -0.036855 -0.086545  \n",
      "20130103  0.026811 -0.183749  \n",
      "20130104  0.024536  0.215032  \n",
      "20130105 -0.025643  0.043548  \n",
      "\n",
      "[5 rows x 2048 columns]\n",
      "(1461, 2048)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "fv_pca = pd.read_csv('../data/fv_pca.zip', compression='zip', index_col=0)\n",
    "print(fv_pca.head())\n",
    "print(fv_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a7f46",
   "metadata": {},
   "source": [
    "## Cleaning and Splitting the Dataset\n",
    "\n",
    "The satellite dataset containing missing data, and hence we need to remove those entry before we put them into the model. Also, we want to split the dataset into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffae8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160101\n",
      "(1095, 2048)\n",
      "(366, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Partition training/testing data\n",
    "index_20160101 = 1095\n",
    "print(events.index[1095])\n",
    "\n",
    "x_train = fv_pca.iloc[:index_20160101,:]\n",
    "x_test = fv_pca.iloc[index_20160101:,:]\n",
    "\n",
    "y_train = events.iloc[:index_20160101, :]\n",
    "y_test = events.iloc[index_20160101:,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0011ad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1082, 2048)\n",
      "(1082, 8)\n",
      "(366, 2048)\n",
      "(366, 8)\n"
     ]
    }
   ],
   "source": [
    "# Drop NA\n",
    "x_train = x_train.dropna(axis=0, how='any')\n",
    "y_train = y_train.loc[x_train.index,:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_test = x_test.dropna(axis=0, how='any')\n",
    "y_test = y_test.loc[x_test.index,:]\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ec144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(x_train, y_train['HRD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f715ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix\n",
      "[[794   0]\n",
      " [  0 288]]\n",
      "Testing Confusion Matrix\n",
      "[[199  39]\n",
      " [ 83  45]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Training CM\n",
    "y_pred = clf.predict(x_train)\n",
    "y_true = y_train['HRD']\n",
    "cm_train = confusion_matrix(y_true, y_pred)\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(cm_train)\n",
    "\n",
    "# Testing CM\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test['HRD']\n",
    "cm_test = confusion_matrix(y_true, y_pred)\n",
    "print(\"Testing Confusion Matrix\")\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c0e97",
   "metadata": {},
   "source": [
    "## For other feature vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514073f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors:../data/fv_cae.zip\n",
      "Event: HRD\n",
      "Original training data shape: (1095, 2048)\n",
      "After dropping NaNs: (1082, 2048)\n",
      "\n",
      "Original testing data shape: (366, 2048)\n",
      "After dropping NaNs: (366, 2048)\n",
      "\n",
      "Fitting GLM:\n",
      "Training Confusion Matrix\n",
      "[[794   0]\n",
      " [  4 284]]\n",
      "Testing Confusion Matrix\n",
      "[[194  44]\n",
      " [ 84  44]]\n"
     ]
    }
   ],
   "source": [
    "# Define feature vector(X) and event(Y)\n",
    "FVPATH = '../data/fv_cae.zip'\n",
    "EVENT = 'HRD'\n",
    "\n",
    "print('Feature Vectors:' + FVPATH)\n",
    "print('Event: ' + EVENT)\n",
    "# Read feature Vector\n",
    "fv = pd.read_csv(FVPATH, compression='zip', index_col=0)\n",
    "\n",
    "# Splitting training/testing\n",
    "x_train = fv.iloc[:index_20160101,:]\n",
    "x_test = fv.iloc[index_20160101:,:]\n",
    "\n",
    "y_train = events.iloc[:index_20160101, :]\n",
    "y_test = events.iloc[index_20160101:,:]\n",
    "\n",
    "# Drop NA\n",
    "print('Original training data shape: ' + str(x_train.shape))\n",
    "x_train = x_train.dropna(axis=0, how='any')\n",
    "y_train = y_train.loc[x_train.index,:]\n",
    "print('After dropping NaNs: ' + str(x_train.shape))\n",
    "\n",
    "print()\n",
    "print('Original testing data shape: ' + str(x_test.shape))\n",
    "x_test = x_test.dropna(axis=0, how='any')\n",
    "y_test = y_test.loc[x_test.index,:]\n",
    "print('After dropping NaNs: ' + str(x_test.shape))\n",
    "\n",
    "print()\n",
    "print('Fitting GLM:')\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(x_train, y_train[EVENT])\n",
    "\n",
    "# Training CM\n",
    "y_pred = clf.predict(x_train)\n",
    "y_true = y_train[EVENT]\n",
    "cm_train = confusion_matrix(y_true, y_pred)\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(cm_train)\n",
    "\n",
    "# Testing CM\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test[EVENT]\n",
    "cm_test = confusion_matrix(y_true, y_pred)\n",
    "print(\"Testing Confusion Matrix\")\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55335d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors:../data/fv_cvae.zip\n",
      "Event: HRD\n",
      "Original training data shape: (1095, 2048)\n",
      "After dropping NaNs: (1082, 2048)\n",
      "\n",
      "Original testing data shape: (366, 2048)\n",
      "After dropping NaNs: (366, 2048)\n",
      "\n",
      "Fitting GLM:\n",
      "Training Confusion Matrix\n",
      "[[794   0]\n",
      " [  0 288]]\n",
      "Testing Confusion Matrix\n",
      "[[182  56]\n",
      " [104  24]]\n"
     ]
    }
   ],
   "source": [
    "# Define feature vector(X) and event(Y)\n",
    "FVPATH = '../data/fv_cvae.zip'\n",
    "EVENT = 'HRD'\n",
    "\n",
    "print('Feature Vectors:' + FVPATH)\n",
    "print('Event: ' + EVENT)\n",
    "\n",
    "# Read feature Vector\n",
    "fv = pd.read_csv(FVPATH, compression='zip', index_col=0)\n",
    "\n",
    "# Splitting training/testing\n",
    "x_train = fv.iloc[:index_20160101,:]\n",
    "x_test = fv.iloc[index_20160101:,:]\n",
    "\n",
    "y_train = events.iloc[:index_20160101, :]\n",
    "y_test = events.iloc[index_20160101:,:]\n",
    "\n",
    "# Drop NA\n",
    "print('Original training data shape: ' + str(x_train.shape))\n",
    "x_train = x_train.dropna(axis=0, how='any')\n",
    "y_train = y_train.loc[x_train.index,:]\n",
    "print('After dropping NaNs: ' + str(x_train.shape))\n",
    "\n",
    "print()\n",
    "print('Original testing data shape: ' + str(x_test.shape))\n",
    "x_test = x_test.dropna(axis=0, how='any')\n",
    "y_test = y_test.loc[x_test.index,:]\n",
    "print('After dropping NaNs: ' + str(x_test.shape))\n",
    "\n",
    "print()\n",
    "print('Fitting GLM:')\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(x_train, y_train[EVENT])\n",
    "\n",
    "# Training CM\n",
    "y_pred = clf.predict(x_train)\n",
    "y_true = y_train[EVENT]\n",
    "cm_train = confusion_matrix(y_true, y_pred)\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(cm_train)\n",
    "\n",
    "# Testing CM\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test[EVENT]\n",
    "cm_test = confusion_matrix(y_true, y_pred)\n",
    "print(\"Testing Confusion Matrix\")\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978a64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors:../data/fv_ptbe.zip\n",
      "Event: HRD\n",
      "Original training data shape: (1095, 2048)\n",
      "After dropping NaNs: (1082, 2048)\n",
      "\n",
      "Original testing data shape: (366, 2048)\n",
      "After dropping NaNs: (366, 2048)\n",
      "\n",
      "Fitting GLM:\n",
      "Training Confusion Matrix\n",
      "[[789   5]\n",
      " [ 58 230]]\n",
      "Testing Confusion Matrix\n",
      "[[195  43]\n",
      " [ 94  34]]\n"
     ]
    }
   ],
   "source": [
    "# Define feature vector(X) and event(Y)\n",
    "FVPATH = '../data/fv_ptbe.zip'\n",
    "EVENT = 'HRD'\n",
    "\n",
    "print('Feature Vectors:' + FVPATH)\n",
    "print('Event: ' + EVENT)\n",
    "\n",
    "# Read feature Vector\n",
    "fv = pd.read_csv(FVPATH, compression='zip', index_col=0)\n",
    "\n",
    "# Splitting training/testing\n",
    "x_train = fv.iloc[:index_20160101,:]\n",
    "x_test = fv.iloc[index_20160101:,:]\n",
    "\n",
    "y_train = events.iloc[:index_20160101, :]\n",
    "y_test = events.iloc[index_20160101:,:]\n",
    "\n",
    "# Drop NA\n",
    "print('Original training data shape: ' + str(x_train.shape))\n",
    "x_train = x_train.dropna(axis=0, how='any')\n",
    "y_train = y_train.loc[x_train.index,:]\n",
    "print('After dropping NaNs: ' + str(x_train.shape))\n",
    "\n",
    "print()\n",
    "print('Original testing data shape: ' + str(x_test.shape))\n",
    "x_test = x_test.dropna(axis=0, how='any')\n",
    "y_test = y_test.loc[x_test.index,:]\n",
    "print('After dropping NaNs: ' + str(x_test.shape))\n",
    "\n",
    "print()\n",
    "print('Fitting GLM:')\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(x_train, y_train[EVENT])\n",
    "\n",
    "# Training CM\n",
    "y_pred = clf.predict(x_train)\n",
    "y_true = y_train[EVENT]\n",
    "cm_train = confusion_matrix(y_true, y_pred)\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(cm_train)\n",
    "\n",
    "# Testing CM\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test[EVENT]\n",
    "cm_test = confusion_matrix(y_true, y_pred)\n",
    "print(\"Testing Confusion Matrix\")\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "234ed60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors:../data/fv_ptin.zip\n",
      "Event: HRD\n",
      "Original training data shape: (1095, 2048)\n",
      "After dropping NaNs: (1082, 2048)\n",
      "\n",
      "Original testing data shape: (366, 2048)\n",
      "After dropping NaNs: (366, 2048)\n",
      "\n",
      "Fitting GLM:\n",
      "Training Confusion Matrix\n",
      "[[794   0]\n",
      " [ 10 278]]\n",
      "Testing Confusion Matrix\n",
      "[[152  86]\n",
      " [ 82  46]]\n"
     ]
    }
   ],
   "source": [
    "# Define feature vector(X) and event(Y)\n",
    "FVPATH = '../data/fv_ptin.zip'\n",
    "EVENT = 'HRD'\n",
    "\n",
    "print('Feature Vectors:' + FVPATH)\n",
    "print('Event: ' + EVENT)\n",
    "\n",
    "# Read feature Vector\n",
    "fv = pd.read_csv(FVPATH, compression='zip', index_col=0)\n",
    "\n",
    "# Splitting training/testing\n",
    "x_train = fv.iloc[:index_20160101,:]\n",
    "x_test = fv.iloc[index_20160101:,:]\n",
    "\n",
    "y_train = events.iloc[:index_20160101, :]\n",
    "y_test = events.iloc[index_20160101:,:]\n",
    "\n",
    "# Drop NA\n",
    "print('Original training data shape: ' + str(x_train.shape))\n",
    "x_train = x_train.dropna(axis=0, how='any')\n",
    "y_train = y_train.loc[x_train.index,:]\n",
    "print('After dropping NaNs: ' + str(x_train.shape))\n",
    "\n",
    "print()\n",
    "print('Original testing data shape: ' + str(x_test.shape))\n",
    "x_test = x_test.dropna(axis=0, how='any')\n",
    "y_test = y_test.loc[x_test.index,:]\n",
    "print('After dropping NaNs: ' + str(x_test.shape))\n",
    "\n",
    "print()\n",
    "print('Fitting GLM:')\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(x_train, y_train[EVENT])\n",
    "\n",
    "# Training CM\n",
    "y_pred = clf.predict(x_train)\n",
    "y_true = y_train[EVENT]\n",
    "cm_train = confusion_matrix(y_true, y_pred)\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(cm_train)\n",
    "\n",
    "# Testing CM\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = y_test[EVENT]\n",
    "cm_test = confusion_matrix(y_true, y_pred)\n",
    "print(\"Testing Confusion Matrix\")\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459a457",
   "metadata": {},
   "source": [
    "For detecting daily precipitation greater or equal than 40-mm, it seems the **GLM-PCA** yields pretty decent results. We will see other events with scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5130d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
